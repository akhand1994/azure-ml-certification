{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating a Batch Inferencing Service\n",
    "\n",
    "In previous labs, you used an Azure ML *pipeline* to automate the training and registration of a model, and you published a model as a web service for real-time *inferencing* (getting predictions from a model). Now you'll combine these two concepts to create a pipeline for *batch inferencing*. What does that mean? Well, imagine a health clinic takes patient measurements all day, saving the details for each patient in a separate file. Then overnight, the diabetes prediction model can be used to process all of the day's patient data as a batch, generating predictions that will be waiting the following morning so that the clinic can follow up with patients who are predicted to be at risk of diabetes. That's what you'll implement in this exercise.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Before you start this lab, ensure that you have completed the *Create an Azure Machine Learning Workspace* and *Create a Compute Instance* tasks in [Lab 1: Getting Started with Azure Machine Learning](./labdocs/Lab01.md). Then open this notebook in Jupyter on your Compute Instance.\n",
    "\n",
    "Let's start by ensuring that you have the latest version of the Azure ML SDK installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-sdk[notebooks,automl,explain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Your Workspace\n",
    "\n",
    "Now connect to your workspace using the Azure ML SDK.\n",
    "\n",
    "> **Note**: You may be prompted to authenticate. Just copy the code and click the link provided to sign into your Azure subscription, and then return to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.18.0 to work with nikhilsuthardp100\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Register a Model\n",
    "\n",
    "Now run the cell below to train and register a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment: diabetes-training\n",
      "Loading Data...\n",
      "Training a decision tree model\n",
      "Accuracy: 0.8893333333333333\n",
      "AUC: 0.8761132394646499\n",
      "Model trained and registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.core import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Create an Azure ML experiment in your workspace\n",
    "experiment = Experiment(workspace = ws, name = \"diabetes-training\")\n",
    "run = experiment.start_logging()\n",
    "print(\"Starting experiment:\", experiment.name)\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a decision tree model\n",
    "print('Training a decision tree model')\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# Save the trained model\n",
    "model_file = 'diabetes_model.pkl'\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "run.upload_file(name = 'outputs/' + model_file, path_or_stream = './' + model_file)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()\n",
    "\n",
    "# Register the model\n",
    "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
    "                   tags={'Training context':'Inline Training'},\n",
    "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
    "\n",
    "print('Model trained and registered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Upload Batch Data\n",
    "\n",
    "Since we don't actually have a fully staffed clinic with patients from whom to get new data for this course, you'll generate a random sample from our diabetes CSV file and use those to test the pipeline. Then you'll upload that data to a datastore in the Azure Machine Learning workspace and register a dataset for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created!\n",
      "Saving files...\n",
      "files saved!\n",
      "Uploading files to datastore...\n",
      "Uploading an estimated of 100 files\n",
      "Uploading batch-data/17.csv\n",
      "Uploaded batch-data/17.csv, 1 files out of an estimated total of 100\n",
      "Uploading batch-data/1.csv\n",
      "Uploaded batch-data/1.csv, 2 files out of an estimated total of 100\n",
      "Uploading batch-data/10.csv\n",
      "Uploaded batch-data/10.csv, 3 files out of an estimated total of 100\n",
      "Uploading batch-data/100.csv\n",
      "Uploaded batch-data/100.csv, 4 files out of an estimated total of 100\n",
      "Uploading batch-data/11.csv\n",
      "Uploaded batch-data/11.csv, 5 files out of an estimated total of 100\n",
      "Uploading batch-data/12.csv\n",
      "Uploaded batch-data/12.csv, 6 files out of an estimated total of 100\n",
      "Uploading batch-data/13.csv\n",
      "Uploaded batch-data/13.csv, 7 files out of an estimated total of 100\n",
      "Uploading batch-data/14.csv\n",
      "Uploaded batch-data/14.csv, 8 files out of an estimated total of 100\n",
      "Uploading batch-data/15.csv\n",
      "Uploaded batch-data/15.csv, 9 files out of an estimated total of 100\n",
      "Uploading batch-data/16.csv\n",
      "Uploaded batch-data/16.csv, 10 files out of an estimated total of 100\n",
      "Uploading batch-data/18.csv\n",
      "Uploaded batch-data/18.csv, 11 files out of an estimated total of 100\n",
      "Uploading batch-data/19.csv\n",
      "Uploaded batch-data/19.csv, 12 files out of an estimated total of 100\n",
      "Uploading batch-data/2.csv\n",
      "Uploaded batch-data/2.csv, 13 files out of an estimated total of 100\n",
      "Uploading batch-data/20.csv\n",
      "Uploaded batch-data/20.csv, 14 files out of an estimated total of 100\n",
      "Uploading batch-data/21.csv\n",
      "Uploaded batch-data/21.csv, 15 files out of an estimated total of 100\n",
      "Uploading batch-data/22.csv\n",
      "Uploaded batch-data/22.csv, 16 files out of an estimated total of 100\n",
      "Uploading batch-data/23.csv\n",
      "Uploaded batch-data/23.csv, 17 files out of an estimated total of 100\n",
      "Uploading batch-data/24.csv\n",
      "Uploaded batch-data/24.csv, 18 files out of an estimated total of 100\n",
      "Uploading batch-data/25.csv\n",
      "Uploaded batch-data/25.csv, 19 files out of an estimated total of 100\n",
      "Uploading batch-data/26.csv\n",
      "Uploaded batch-data/26.csv, 20 files out of an estimated total of 100\n",
      "Uploading batch-data/27.csv\n",
      "Uploaded batch-data/27.csv, 21 files out of an estimated total of 100\n",
      "Uploading batch-data/28.csv\n",
      "Uploaded batch-data/28.csv, 22 files out of an estimated total of 100\n",
      "Uploading batch-data/29.csv\n",
      "Uploaded batch-data/29.csv, 23 files out of an estimated total of 100\n",
      "Uploading batch-data/3.csv\n",
      "Uploaded batch-data/3.csv, 24 files out of an estimated total of 100\n",
      "Uploading batch-data/30.csv\n",
      "Uploaded batch-data/30.csv, 25 files out of an estimated total of 100\n",
      "Uploading batch-data/31.csv\n",
      "Uploaded batch-data/31.csv, 26 files out of an estimated total of 100\n",
      "Uploading batch-data/32.csv\n",
      "Uploaded batch-data/32.csv, 27 files out of an estimated total of 100\n",
      "Uploading batch-data/33.csv\n",
      "Uploaded batch-data/33.csv, 28 files out of an estimated total of 100\n",
      "Uploading batch-data/34.csv\n",
      "Uploaded batch-data/34.csv, 29 files out of an estimated total of 100\n",
      "Uploading batch-data/35.csv\n",
      "Uploaded batch-data/35.csv, 30 files out of an estimated total of 100\n",
      "Uploading batch-data/36.csv\n",
      "Uploaded batch-data/36.csv, 31 files out of an estimated total of 100\n",
      "Uploading batch-data/37.csv\n",
      "Uploaded batch-data/37.csv, 32 files out of an estimated total of 100\n",
      "Uploading batch-data/38.csv\n",
      "Uploaded batch-data/38.csv, 33 files out of an estimated total of 100\n",
      "Uploading batch-data/39.csv\n",
      "Uploaded batch-data/39.csv, 34 files out of an estimated total of 100\n",
      "Uploading batch-data/4.csv\n",
      "Uploaded batch-data/4.csv, 35 files out of an estimated total of 100\n",
      "Uploading batch-data/40.csv\n",
      "Uploaded batch-data/40.csv, 36 files out of an estimated total of 100\n",
      "Uploading batch-data/41.csv\n",
      "Uploaded batch-data/41.csv, 37 files out of an estimated total of 100\n",
      "Uploading batch-data/42.csv\n",
      "Uploaded batch-data/42.csv, 38 files out of an estimated total of 100\n",
      "Uploading batch-data/43.csv\n",
      "Uploaded batch-data/43.csv, 39 files out of an estimated total of 100\n",
      "Uploading batch-data/44.csv\n",
      "Uploaded batch-data/44.csv, 40 files out of an estimated total of 100\n",
      "Uploading batch-data/45.csv\n",
      "Uploaded batch-data/45.csv, 41 files out of an estimated total of 100\n",
      "Uploading batch-data/46.csv\n",
      "Uploaded batch-data/46.csv, 42 files out of an estimated total of 100\n",
      "Uploading batch-data/47.csv\n",
      "Uploaded batch-data/47.csv, 43 files out of an estimated total of 100\n",
      "Uploading batch-data/48.csv\n",
      "Uploaded batch-data/48.csv, 44 files out of an estimated total of 100\n",
      "Uploading batch-data/49.csv\n",
      "Uploaded batch-data/49.csv, 45 files out of an estimated total of 100\n",
      "Uploading batch-data/5.csv\n",
      "Uploaded batch-data/5.csv, 46 files out of an estimated total of 100\n",
      "Uploading batch-data/50.csv\n",
      "Uploaded batch-data/50.csv, 47 files out of an estimated total of 100\n",
      "Uploading batch-data/51.csv\n",
      "Uploaded batch-data/51.csv, 48 files out of an estimated total of 100\n",
      "Uploading batch-data/52.csv\n",
      "Uploaded batch-data/52.csv, 49 files out of an estimated total of 100\n",
      "Uploading batch-data/54.csv\n",
      "Uploaded batch-data/54.csv, 50 files out of an estimated total of 100\n",
      "Uploading batch-data/55.csv\n",
      "Uploaded batch-data/55.csv, 51 files out of an estimated total of 100\n",
      "Uploading batch-data/56.csv\n",
      "Uploaded batch-data/56.csv, 52 files out of an estimated total of 100\n",
      "Uploading batch-data/57.csv\n",
      "Uploaded batch-data/57.csv, 53 files out of an estimated total of 100\n",
      "Uploading batch-data/58.csv\n",
      "Uploaded batch-data/58.csv, 54 files out of an estimated total of 100\n",
      "Uploading batch-data/6.csv\n",
      "Uploaded batch-data/6.csv, 55 files out of an estimated total of 100\n",
      "Uploading batch-data/60.csv\n",
      "Uploaded batch-data/60.csv, 56 files out of an estimated total of 100\n",
      "Uploading batch-data/61.csv\n",
      "Uploaded batch-data/61.csv, 57 files out of an estimated total of 100\n",
      "Uploading batch-data/62.csv\n",
      "Uploaded batch-data/62.csv, 58 files out of an estimated total of 100\n",
      "Uploading batch-data/63.csv\n",
      "Uploaded batch-data/63.csv, 59 files out of an estimated total of 100\n",
      "Uploading batch-data/64.csv\n",
      "Uploaded batch-data/64.csv, 60 files out of an estimated total of 100\n",
      "Uploading batch-data/65.csv\n",
      "Uploaded batch-data/65.csv, 61 files out of an estimated total of 100\n",
      "Uploading batch-data/66.csv\n",
      "Uploaded batch-data/66.csv, 62 files out of an estimated total of 100\n",
      "Uploading batch-data/67.csv\n",
      "Uploaded batch-data/67.csv, 63 files out of an estimated total of 100\n",
      "Uploading batch-data/68.csv\n",
      "Uploaded batch-data/68.csv, 64 files out of an estimated total of 100\n",
      "Uploading batch-data/69.csv\n",
      "Uploaded batch-data/69.csv, 65 files out of an estimated total of 100\n",
      "Uploading batch-data/53.csv\n",
      "Uploaded batch-data/53.csv, 66 files out of an estimated total of 100\n",
      "Uploading batch-data/59.csv\n",
      "Uploaded batch-data/59.csv, 67 files out of an estimated total of 100\n",
      "Uploading batch-data/7.csv\n",
      "Uploaded batch-data/7.csv, 68 files out of an estimated total of 100\n",
      "Uploading batch-data/70.csv\n",
      "Uploaded batch-data/70.csv, 69 files out of an estimated total of 100\n",
      "Uploading batch-data/71.csv\n",
      "Uploaded batch-data/71.csv, 70 files out of an estimated total of 100\n",
      "Uploading batch-data/72.csv\n",
      "Uploaded batch-data/72.csv, 71 files out of an estimated total of 100\n",
      "Uploading batch-data/73.csv\n",
      "Uploaded batch-data/73.csv, 72 files out of an estimated total of 100\n",
      "Uploading batch-data/74.csv\n",
      "Uploaded batch-data/74.csv, 73 files out of an estimated total of 100\n",
      "Uploading batch-data/75.csv\n",
      "Uploaded batch-data/75.csv, 74 files out of an estimated total of 100\n",
      "Uploading batch-data/76.csv\n",
      "Uploaded batch-data/76.csv, 75 files out of an estimated total of 100\n",
      "Uploading batch-data/77.csv\n",
      "Uploaded batch-data/77.csv, 76 files out of an estimated total of 100\n",
      "Uploading batch-data/78.csv\n",
      "Uploaded batch-data/78.csv, 77 files out of an estimated total of 100\n",
      "Uploading batch-data/79.csv\n",
      "Uploaded batch-data/79.csv, 78 files out of an estimated total of 100\n",
      "Uploading batch-data/8.csv\n",
      "Uploaded batch-data/8.csv, 79 files out of an estimated total of 100\n",
      "Uploading batch-data/80.csv\n",
      "Uploaded batch-data/80.csv, 80 files out of an estimated total of 100\n",
      "Uploading batch-data/81.csv\n",
      "Uploaded batch-data/81.csv, 81 files out of an estimated total of 100\n",
      "Uploading batch-data/82.csv\n",
      "Uploaded batch-data/82.csv, 82 files out of an estimated total of 100\n",
      "Uploading batch-data/83.csv\n",
      "Uploaded batch-data/83.csv, 83 files out of an estimated total of 100\n",
      "Uploading batch-data/84.csv\n",
      "Uploaded batch-data/84.csv, 84 files out of an estimated total of 100\n",
      "Uploading batch-data/85.csv\n",
      "Uploaded batch-data/85.csv, 85 files out of an estimated total of 100\n",
      "Uploading batch-data/86.csv\n",
      "Uploaded batch-data/86.csv, 86 files out of an estimated total of 100\n",
      "Uploading batch-data/87.csv\n",
      "Uploaded batch-data/87.csv, 87 files out of an estimated total of 100\n",
      "Uploading batch-data/88.csv\n",
      "Uploaded batch-data/88.csv, 88 files out of an estimated total of 100\n",
      "Uploading batch-data/89.csv\n",
      "Uploaded batch-data/89.csv, 89 files out of an estimated total of 100\n",
      "Uploading batch-data/9.csv\n",
      "Uploaded batch-data/9.csv, 90 files out of an estimated total of 100\n",
      "Uploading batch-data/90.csv\n",
      "Uploaded batch-data/90.csv, 91 files out of an estimated total of 100\n",
      "Uploading batch-data/91.csv\n",
      "Uploaded batch-data/91.csv, 92 files out of an estimated total of 100\n",
      "Uploading batch-data/92.csv\n",
      "Uploaded batch-data/92.csv, 93 files out of an estimated total of 100\n",
      "Uploading batch-data/93.csv\n",
      "Uploaded batch-data/93.csv, 94 files out of an estimated total of 100\n",
      "Uploading batch-data/94.csv\n",
      "Uploaded batch-data/94.csv, 95 files out of an estimated total of 100\n",
      "Uploading batch-data/95.csv\n",
      "Uploaded batch-data/95.csv, 96 files out of an estimated total of 100\n",
      "Uploading batch-data/96.csv\n",
      "Uploaded batch-data/96.csv, 97 files out of an estimated total of 100\n",
      "Uploading batch-data/97.csv\n",
      "Uploaded batch-data/97.csv, 98 files out of an estimated total of 100\n",
      "Uploading batch-data/98.csv\n",
      "Uploaded batch-data/98.csv, 99 files out of an estimated total of 100\n",
      "Uploading batch-data/99.csv\n",
      "Uploaded batch-data/99.csv, 100 files out of an estimated total of 100\n",
      "Uploaded 100 files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the diabetes data\n",
    "diabetes = pd.read_csv('data/diabetes2.csv')\n",
    "# Get a 100-item sample of the feature columns (not the diabetic label)\n",
    "sample = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].sample(n=100).values\n",
    "\n",
    "# Create a folder\n",
    "batch_folder = './batch-data'\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "print(\"Folder created!\")\n",
    "\n",
    "# Save each sample as a separate file\n",
    "print(\"Saving files...\")\n",
    "for i in range(100):\n",
    "    fname = str(i+1) + '.csv'\n",
    "    sample[i].tofile(os.path.join(batch_folder, fname), sep=\",\")\n",
    "print(\"files saved!\")\n",
    "\n",
    "# Upload the files to the default datastore\n",
    "print(\"Uploading files to datastore...\")\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\n",
    "\n",
    "# Register a dataset for the input data\n",
    "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\n",
    "try:\n",
    "    batch_data_set = batch_data_set.register(workspace=ws, \n",
    "                                             name='batch-data',\n",
    "                                             description='batch data',\n",
    "                                             create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute\n",
    "\n",
    "We'll need a compute context for the pipeline, so we'll create an Azure Machine Learning compute cluster in your workspace (or use an existing one if you have created it previously).\n",
    "\n",
    "> **Important**: Change *your-compute-cluster* to a unique name for your compute cluster in the code below before running it! Cluster names must be globally unique names between 2 to 16 characters in length. Valid characters are letters, digits, and the - character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"nikhilvmcluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        inference_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline for Batch Inferencing\n",
    "\n",
    "Now we're ready to define the pipeline we'll use for batch inferencing. Our pipeline will need Python code to perform the batch inferencing, so let's create a folder where we can keep all the files used by the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = 'batch_pipeline'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a Python script to do the actual work, and save it in the pipeline folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch_pipeline/batch_diabetes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/batch_diabetes.py\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('diabetes_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read the comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define a run context that includes the dependencies required by the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration ready.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "# Add dependencies required by the model\n",
    "# For scikit-learn models, you need scikit-learn\n",
    "# For parallel pipeline steps, you need azureml-core and azureml-dataprep[fuse]\n",
    "cd = CondaDependencies.create(pip_packages=['scikit-learn','azureml-defaults','azureml-core','azureml-dataprep[fuse]'])\n",
    "\n",
    "batch_env = Environment(name='batch_environment')\n",
    "batch_env.python.conda_dependencies = cd\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "print('Configuration ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to use a pipeline to run the batch prediction script, generate predictions from the input data, and save the results as a text file in the output folder. To do this, you can use a **ParallelRunStep**, which enables the batch data to be processed in parallel and the results collated in a single output file named *parallel_run_step.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "output_dir = PipelineData(name='inferences', \n",
    "                          datastore=default_ds, \n",
    "                          output_path_on_compute='diabetes/results')\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    entry_script=\"batch_diabetes.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=inference_cluster,\n",
    "    node_count=2)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score-diabetes',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print('Steps defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to put the step into a pipeline, and run it.\n",
    "\n",
    "> **Note**: This may take some time! You can monitor the experiment run in [Azure Machine Learning studio](https://ml.azure.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step batch-score-diabetes [e3a7cbed][ba9712e0-6e09-4cc4-b589-67720d23bf7d], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 99449476-8765-452c-a28f-e734e6f92a0a\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch_prediction_pipeline/runs/99449476-8765-452c-a28f-e734e6f92a0a?wsid=/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourcegroups/nikhil-suthardp100/workspaces/nikhilsuthardp100\n",
      "Running pipeline...\n",
      "PipelineRunId: 99449476-8765-452c-a28f-e734e6f92a0a\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch_prediction_pipeline/runs/99449476-8765-452c-a28f-e734e6f92a0a?wsid=/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourcegroups/nikhil-suthardp100/workspaces/nikhilsuthardp100\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 9212ba86-c3d2-4914-89e8-68de3eacc10c\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch_prediction_pipeline/runs/9212ba86-c3d2-4914-89e8-68de3eacc10c?wsid=/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourcegroups/nikhil-suthardp100/workspaces/nikhilsuthardp100\n",
      "StepRun( batch-score-diabetes ) Status: NotStarted\n",
      "StepRun( batch-score-diabetes ) Status: Queued\n",
      "StepRun( batch-score-diabetes ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "2020/11/25 11:07:50 Downloading source code...\n",
      "2020/11/25 11:07:52 Finished downloading source code\n",
      "2020/11/25 11:07:52 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2020/11/25 11:07:53 Successfully set up Docker network: acb_default_network\n",
      "2020/11/25 11:07:53 Setting up Docker configuration...\n",
      "2020/11/25 11:07:53 Successfully set up Docker configuration\n",
      "2020/11/25 11:07:53 Logging in to registry: ed4462f89ccc45679b9210c6758878a5.azurecr.io\n",
      "2020/11/25 11:07:54 Successfully logged into ed4462f89ccc45679b9210c6758878a5.azurecr.io\n",
      "2020/11/25 11:07:54 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2020/11/25 11:07:54 Scanning for dependencies...\n",
      "2020/11/25 11:07:55 Successfully scanned dependencies\n",
      "2020/11/25 11:07:55 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon     64kB\n",
      "\n",
      "Step 1/15 : FROM mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1@sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99\n",
      "sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99: Pulling from azureml/intelmpi2018.3-ubuntu16.04\n",
      "Digest: sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1@sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99\n",
      " ---> 287916b809d9\n",
      "Step 2/15 : USER root\n",
      " ---> Running in 1da2d0979651\n",
      "Removing intermediate container 1da2d0979651\n",
      " ---> f99c4b10efdf\n",
      "Step 3/15 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in a6884da53564\n",
      "Removing intermediate container a6884da53564\n",
      " ---> 900e7c0b7a6d\n",
      "Step 4/15 : WORKDIR /\n",
      " ---> Running in 463fc469326b\n",
      "Removing intermediate container 463fc469326b\n",
      " ---> 9b67f2f578f3\n",
      "Step 5/15 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 395f1a242caf\n",
      "Step 6/15 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in 74bac77cc2ca\n",
      "Removing intermediate container 74bac77cc2ca\n",
      " ---> 841c8b5bf79d\n",
      "Step 7/15 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> 08b7c69e76bf\n",
      "Step 8/15 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373 -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in 0f983070c0ea\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... \n",
      "done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "libgcc-ng-9.1.0      | 8.1 MB    |            |   0% \n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ##5        |  25% \n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n",
      "\n",
      "xz-5.2.5             | 438 KB    |            |   0% \n",
      "xz-5.2.5             | 438 KB    | ########## | 100% \n",
      "\n",
      "libedit-3.1          | 171 KB    |            |   0% \n",
      "libedit-3.1          | 171 KB    | ########## | 100% \n",
      "\n",
      "tk-8.6.10            | 3.2 MB    |            |   0% \n",
      "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
      "\n",
      "zlib-1.2.11          | 120 KB    |            |   0% \n",
      "zlib-1.2.11          | 120 KB    | ########## | 100% \n",
      "\n",
      "certifi-2020.6.20    | 160 KB    |            |   0% \n",
      "certifi-2020.6.20    | 160 KB    | ########## | 100% \n",
      "\n",
      "readline-7.0         | 387 KB    |            |   0% \n",
      "readline-7.0         | 387 KB    | ########## | 100% \n",
      "\n",
      "wheel-0.35.1         | 36 KB     |            |   0% \n",
      "wheel-0.35.1         | 36 KB     | ########## | 100% \n",
      "\n",
      "ca-certificates-2020 | 128 KB    |            |   0% \n",
      "ca-certificates-2020 | 128 KB    | ########## | 100% \n",
      "\n",
      "pip-20.2.4           | 2.0 MB    |            |   0% \n",
      "pip-20.2.4           | 2.0 MB    | ########## | 100% \n",
      "\n",
      "python-3.6.2         | 27.0 MB   |            |   0% \n",
      "python-3.6.2         | 27.0 MB   | ##1        |  21% \n",
      "python-3.6.2         | 27.0 MB   | ######2    |  62% \n",
      "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
      "\n",
      "openssl-1.0.2u       | 3.1 MB    |            |   0% \n",
      "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
      "\n",
      "ncurses-6.0          | 907 KB    |            |   0% \n",
      "ncurses-6.0          | 907 KB    | ########## | 100% \n",
      "\n",
      "libffi-3.2.1         | 52 KB     |            |   0% \n",
      "libffi-3.2.1         | 52 KB     | ########## | 100% \n",
      "\n",
      "sqlite-3.23.1        | 1.5 MB    |            |   0% \n",
      "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
      "\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    |            |   0% \n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
      "\n",
      "setuptools-50.3.0    | 891 KB    |            |   0% \n",
      "setuptools-50.3.0    | 891 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Ran pip subprocess with arguments:\n",
      "['/azureml-envs/azureml_be341871f2a1af415628bd3b45b58373/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.yznlckmk.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting azureml-defaults~=1.18.0\n",
      "  Downloading azureml_defaults-1.18.0-py3-none-any.whl (3.1 kB)\n",
      "Collecting azureml-core~=1.18.0\n",
      "  Downloading azureml_core-1.18.0.post3-py3-none-any.whl (2.1 MB)\n",
      "Collecting azureml-dataprep[fuse]\n",
      "  Downloading azureml_dataprep-2.6.0-py3-none-any.whl (39.4 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting numpy>=1.13.3\n",
      "  Downloading numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting applicationinsights>=0.11.7\n",
      "  Downloading applicationinsights-0.11.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting flask==1.0.3\n",
      "  Downloading Flask-1.0.3-py2.py3-none-any.whl (92 kB)\n",
      "Collecting json-logging-py==0.2\n",
      "  Downloading json-logging-py-0.2.tar.gz (3.6 kB)\n",
      "Collecting azureml-model-management-sdk==1.0.1b6.post1\n",
      "  Downloading azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130 kB)\n",
      "Collecting gunicorn==19.9.0\n",
      "  Downloading gunicorn-19.9.0-py2.py3-none-any.whl (112 kB)\n",
      "Collecting azureml-dataset-runtime[fuse]~=1.18.0\n",
      "  Downloading azureml_dataset_runtime-1.18.0-py3-none-any.whl (3.4 kB)\n",
      "Collecting werkzeug<=1.0.1,>=0.16.1\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting configparser==3.7.4\n",
      "  Downloading configparser-3.7.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting azure-graphrbac<1.0.0,>=0.40.0\n",
      "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
      "Collecting contextlib2\n",
      "  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting azure-mgmt-keyvault<7.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_keyvault-2.2.0-py2.py3-none-any.whl (89 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "Collecting azure-mgmt-storage<16.0.0,>=1.5.0\n",
      "  Downloading azure_mgmt_storage-11.2.0-py2.py3-none-any.whl (547 kB)\n",
      "Collecting adal>=1.2.0\n",
      "  Downloading adal-1.2.5-py2.py3-none-any.whl (55 kB)\n",
      "Collecting docker\n",
      "  Downloading docker-4.4.0-py2.py3-none-any.whl (146 kB)\n",
      "Collecting PyJWT<2.0.0\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pathspec\n",
      "  Downloading pathspec-0.8.1-py2.py3-none-any.whl (28 kB)\n",
      "Collecting jmespath\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting msrest>=0.5.1\n",
      "  Downloading msrest-0.6.19-py2.py3-none-any.whl (84 kB)\n",
      "Collecting SecretStorage\n",
      "  Downloading SecretStorage-3.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting backports.tempfile\n",
      "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting pyopenssl<20.0.0\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting ndg-httpsclient\n",
      "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting requests>=2.19.1\n",
      "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting azure-mgmt-authorization<1.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\n",
      "Collecting azure-common>=1.1.12\n",
      "  Downloading azure_common-1.1.26-py2.py3-none-any.whl (12 kB)\n",
      "Collecting ruamel.yaml>=0.15.35\n",
      "  Downloading ruamel.yaml-0.16.12-py2.py3-none-any.whl (111 kB)\n",
      "Collecting jsonpickle\n",
      "  Downloading jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*\n",
      "  Downloading cryptography-3.2.1-cp35-abi3-manylinux2010_x86_64.whl (2.6 MB)\n",
      "Collecting msrestazure>=0.4.33\n",
      "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
      "Collecting azure-mgmt-containerregistry>=2.0.0\n",
      "  Downloading azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718 kB)\n",
      "Collecting urllib3>=1.23\n",
      "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "Collecting azure-mgmt-resource<15.0.0,>=1.2.1\n",
      "  Downloading azure_mgmt_resource-10.3.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Collecting azure-identity<1.5.0,>=1.2.0\n",
      "  Downloading azure_identity-1.4.1-py2.py3-none-any.whl (86 kB)\n",
      "Collecting dotnetcore2<3.0.0,>=2.1.14\n",
      "  Downloading dotnetcore2-2.1.19-py3-none-manylinux1_x86_64.whl (28.7 MB)\n",
      "Collecting azureml-dataprep-rslex<1.5.0a,>=1.4.0dev0\n",
      "  Downloading azureml_dataprep_rslex-1.4.0-cp36-cp36m-manylinux2010_x86_64.whl (7.9 MB)\n",
      "Collecting azureml-dataprep-native<27.0.0,>=26.0.0\n",
      "  Downloading azureml_dataprep_native-26.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting cloudpickle<2.0.0,>=1.1.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting fusepy<4.0.0,>=3.0.1; extra == \"fuse\"\n",
      "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting Jinja2>=2.10\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting liac-arff>=2.1.1\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "Collecting dill>=0.2.7.1\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Collecting six>=1.10\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pandas>=0.20.2\n",
      "  Downloading pandas-1.1.4-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting pyarrow<2.0.0,>=0.17.0\n",
      "  Downloading pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-core~=1.18.0->-r /azureml-environment-setup/condaenv.yznlckmk.requirements.txt (line 3)) (2020.6.20)\n",
      "Collecting jeepney>=0.6\n",
      "  Downloading jeepney-0.6.0-py3-none-any.whl (45 kB)\n",
      "Collecting backports.weakref\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
      "  Downloading ruamel.yaml.clib-0.2.2-cp36-cp36m-manylinux1_x86_64.whl (549 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-3.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Downloading cffi-1.14.4-cp36-cp36m-manylinux1_x86_64.whl (401 kB)\n",
      "Collecting azure-core<2.0.0,>=1.0.0\n",
      "  Downloading azure_core-1.9.0-py2.py3-none-any.whl (124 kB)\n",
      "Collecting msal-extensions~=0.2.2\n",
      "  Downloading msal_extensions-0.2.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting msal<2.0.0,>=1.3.0\n",
      "  Downloading msal-1.6.0-py2.py3-none-any.whl (50 kB)\n",
      "Collecting distro>=1.2.0\n",
      "  Downloading distro-1.5.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting portalocker~=1.0; platform_system != \"Windows\"\n",
      "  Downloading portalocker-1.7.1-py2.py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: json-logging-py, fusepy, liac-arff\n",
      "  Building wheel for json-logging-py (setup.py): started\n",
      "  Building wheel for json-logging-py (setup.py): finished with status 'done'\n",
      "  Created wheel for json-logging-py: filename=json_logging_py-0.2-py3-none-any.whl size=3924 sha256=00c5c456d3efb7fddcdebaaf6a7bd79e1fe10f115173d09dd26f038121d8ff5c\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/1d/52/535a274b9c2ce7d4064838f2bdb62013801281ef7d7f21e2ee\n",
      "  Building wheel for fusepy (setup.py): started\n",
      "  Building wheel for fusepy (setup.py): finished with status 'done'\n",
      "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10504 sha256=3b46c982057522394d662ac4a2316d673cb940a47c1592606d6c1c9445d76e36\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/5c/83/1dd7e8a232d12227e5410120f4374b33adeb4037473105b079\n",
      "  Building wheel for liac-arff (setup.py): started\n",
      "  Building wheel for liac-arff (setup.py): finished with status 'done'\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11730 sha256=a30c8e712e4dc12eb2150f8a5d1368531ce9d6b2e3774ee2af4c2fb662690f1a\n",
      "  Stored in directory: /root/.cache/pip/wheels/53/ba/da/8562a6a6dbb428fd1ecc21053106df3948645cd991958f669b\n",
      "Successfully built json-logging-py fusepy liac-arff\n",
      "Installing collected packages: joblib, numpy, scipy, threadpoolctl, scikit-learn, applicationinsights, werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask, json-logging-py, six, urllib3, chardet, idna, requests, PyJWT, python-dateutil, pycparser, cffi, cryptography, adal, isodate, oauthlib, requests-oauthlib, msrest, msrestazure, azure-common, azure-graphrbac, contextlib2, azure-mgmt-keyvault, pytz, azure-mgmt-storage, websocket-client, docker, pathspec, jmespath, jeepney, SecretStorage, backports.weakref, backports.tempfile, pyopenssl, pyasn1, ndg-httpsclient, azure-mgmt-authorization, ruamel.yaml.clib, ruamel.yaml, zipp, importlib-metadata, jsonpickle, azure-mgmt-containerregistry, azure-mgmt-resource, azureml-core, liac-arff, dill, pandas, azureml-model-management-sdk, gunicorn, azure-core, portalocker, msal, msal-extensions, azure-identity, distro, dotnetcore2, azureml-dataprep-rslex, azureml-dataprep-native, cloudpickle, fusepy, azureml-dataprep, pyarrow, azureml-dataset-runtime, configparser, azureml-defaults\n",
      "Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 PyJWT-1.7.1 SecretStorage-3.3.0 adal-1.2.5 applicationinsights-0.11.9 azure-common-1.1.26 azure-core-1.9.0 azure-graphrbac-0.61.1 azure-identity-1.4.1 azure-mgmt-authorization-0.61.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-2.2.0 azure-mgmt-resource-10.3.0 azure-mgmt-storage-11.2.0 azureml-core-1.18.0.post3 azureml-dataprep-2.6.0 azureml-dataprep-native-26.0.0 azureml-dataprep-rslex-1.4.0 azureml-dataset-runtime-1.18.0 azureml-defaults-1.18.0 azureml-model-management-sdk-1.0.1b6.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 cffi-1.14.4 chardet-3.0.4 click-7.1.2 cloudpickle-1.6.0 configparser-3.7.4 contextlib2-0.6.0.post1 cryptography-3.2.1 dill-0.3.3 distro-1.5.0 docker-4.4.0 dotnetcore2-2.1.19 flask-1.0.3 fusepy-3.0.1 gunicorn-19.9.0 idna-2.10 importlib-metadata-3.1.0 isodate-0.6.0 itsdangerous-1.1.0 jeepney-0.6.0 jmespath-0.10.0 joblib-0.17.0 json-logging-py-0.2 jsonpickle-1.4.1 liac-arff-2.5.0 msal-1.6.0 msal-extensions-0.2.2 msrest-0.6.19 msrestazure-0.6.4 ndg-httpsclient-0.5.1 numpy-1.19.4 oauthlib-3.1.0 pandas-1.1.4 pathspec-0.8.1 portalocker-1.7.1 pyarrow-1.0.1 pyasn1-0.4.8 pycparser-2.20 pyopenssl-19.1.0 python-dateutil-2.8.1 pytz-2020.4 requests-2.25.0 requests-oauthlib-1.3.0 ruamel.yaml-0.16.12 ruamel.yaml.clib-0.2.2 scikit-learn-0.23.2 scipy-1.5.4 six-1.15.0 threadpoolctl-2.1.0 urllib3-1.26.2 websocket-client-0.57.0 werkzeug-1.0.1 zipp-3.4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\u001b[0mWARNING: /root/.conda/pkgs does not exist\n",
      "Removing intermediate container 0f983070c0ea\n",
      " ---> 4d538f17b8ea\n",
      "Step 9/15 : ENV PATH /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373/bin:$PATH\n",
      " ---> Running in 60eda27ca390\n",
      "Removing intermediate container 60eda27ca390\n",
      " ---> ee0f7c4802f9\n",
      "Step 10/15 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373\n",
      " ---> Running in 27b906bd50f0\n",
      "Removing intermediate container 27b906bd50f0\n",
      " ---> c2c439fa3602\n",
      "Step 11/15 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in 13492735998c\n",
      "Removing intermediate container 13492735998c\n",
      " ---> 69503e2b9d9a\n",
      "Step 12/15 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
      " ---> 83c2d1a1ecbf\n",
      "Step 13/15 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\n",
      " ---> Running in af4c396e6522\n",
      "Removing intermediate container af4c396e6522\n",
      " ---> 91380a2a3321\n",
      "Step 14/15 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in b0ebb20a0a4f\n",
      "Removing intermediate container b0ebb20a0a4f\n",
      " ---> 1816d20ddda4\n",
      "Step 15/15 : CMD [\"bash\"]\n",
      " ---> Running in b5eb5b600377\n",
      "Removing intermediate container b5eb5b600377\n",
      " ---> 20e592bdcee2\n",
      "Successfully built 20e592bdcee2\n",
      "Successfully tagged ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb:latest\n",
      "2020/11/25 11:10:18 Successfully executed container: acb_step_0\n",
      "2020/11/25 11:10:18 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2020/11/25 11:10:18 Pushing image: ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb:latest, attempt 1\n",
      "The push refers to repository [ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb]\n",
      "88e2b28c51a3: Preparing\n",
      "baf819b17dc6: Preparing\n",
      "769b7ffcb68a: Preparing\n",
      "d7e473c94fad: Preparing\n",
      "db7d749339bd: Preparing\n",
      "89380a0d373c: Preparing\n",
      "13e378616f24: Preparing\n",
      "efc99d952c3d: Preparing\n",
      "9e292a80b88a: Preparing\n",
      "5e1805eb9eb5: Preparing\n",
      "8dab94e6d05c: Preparing\n",
      "2817caf0a082: Preparing\n",
      "aece08fd27fc: Preparing\n",
      "4caea5ef1f0b: Preparing\n",
      "dcc0cc99372e: Preparing\n",
      "87c128261339: Preparing\n",
      "41a253a417e6: Preparing\n",
      "e06660e80cf4: Preparing\n",
      "2817caf0a082: Waiting\n",
      "89380a0d373c: Waiting\n",
      "13e378616f24: Waiting\n",
      "efc99d952c3d: Waiting\n",
      "9e292a80b88a: Waiting\n",
      "5e1805eb9eb5: Waiting\n",
      "8dab94e6d05c: Waiting\n",
      "aece08fd27fc: Waiting\n",
      "4caea5ef1f0b: Waiting\n",
      "dcc0cc99372e: Waiting\n",
      "87c128261339: Waiting\n",
      "41a253a417e6: Waiting\n",
      "e06660e80cf4: Waiting\n",
      "db7d749339bd: Pushed\n",
      "769b7ffcb68a: Pushed\n",
      "d7e473c94fad: Pushed\n",
      "88e2b28c51a3: Pushed\n",
      "13e378616f24: Pushed\n",
      "89380a0d373c: Pushed\n",
      "9e292a80b88a: Pushed\n",
      "efc99d952c3d: Pushed\n",
      "\n",
      "aece08fd27fc: Pushed\n",
      "2817caf0a082: Pushed\n",
      "dcc0cc99372e: Pushed\n",
      "87c128261339: Pushed\n",
      "41a253a417e6: Pushed\n",
      "5e1805eb9eb5: Pushed\n",
      "8dab94e6d05c: Pushed\n",
      "\n",
      "e06660e80cf4: Pushed\n",
      "baf819b17dc6: Pushed\n",
      "4caea5ef1f0b: Pushed\n",
      "latest: digest: sha256:b8a80737a8a9923b7bc15781fcfd1ebe16c4fada68246bc4019af6d7af13fa27 size: 4095\n",
      "2020/11/25 11:12:04 Successfully pushed image: ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb:latest\n",
      "2020/11/25 11:12:04 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 143.437991)\n",
      "2020/11/25 11:12:04 Populating digests for step ID: acb_step_0...\n",
      "2020/11/25 11:12:06 Successfully populated digests for step ID: acb_step_0\n",
      "2020/11/25 11:12:06 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 106.601750)\n",
      "2020/11/25 11:12:06 The following dependencies were found:\n",
      "2020/11/25 11:12:06 \n",
      "- image:\n",
      "    registry: ed4462f89ccc45679b9210c6758878a5.azurecr.io\n",
      "    repository: azureml/azureml_204af3dadda12064de42b82ef2062eeb\n",
      "    tag: latest\n",
      "    digest: sha256:b8a80737a8a9923b7bc15781fcfd1ebe16c4fada68246bc4019af6d7af13fa27\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/intelmpi2018.3-ubuntu16.04\n",
      "    tag: 20200821.v1\n",
      "    digest: sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99\n",
      "  git: {}\n",
      "\n",
      "\n",
      "Run ID: ch4 was successful after 4m17s\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt\n",
      "========================================================================================================================\n",
      "2020-11-25T11:16:33Z Starting output-watcher...\n",
      "2020-11-25T11:16:33Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt\n",
      "========================================================================================================================\n",
      "2020-11-25T11:16:38Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_204af3dadda12064de42b82ef2062eeb\n",
      "8e097b52bfb8: Pulling fs layer\n",
      "a613a9b4553c: Pulling fs layer\n",
      "acc000f01536: Pulling fs layer\n",
      "73eef93b7466: Pulling fs layer\n",
      "d5a54c1fb97f: Pulling fs layer\n",
      "1536f6ca931b: Pulling fs layer\n",
      "d7b631d130cb: Pulling fs layer\n",
      "75ffe8dfb222: Pulling fs layer\n",
      "86b4bf2f8d5f: Pulling fs layer\n",
      "5335952fa8d3: Pulling fs layer\n",
      "96fa3cc6fe10: Pulling fs layer\n",
      "e428dd9daa94: Pulling fs layer\n",
      "1903eafd18ff: Pulling fs layer\n",
      "3d15922e3194: Pulling fs layer\n",
      "7a5cad2d557c: Pulling fs layer\n",
      "21b426cb4dd6: Pulling fs layer\n",
      "6ecb67338134: Pulling fs layer\n",
      "0b98575ca820: Pulling fs layer\n",
      "5335952fa8d3: Waiting\n",
      "96fa3cc6fe10: Waiting\n",
      "e428dd9daa94: Waiting\n",
      "1903eafd18ff: Waiting\n",
      "3d15922e3194: Waiting\n",
      "7a5cad2d557c: Waiting\n",
      "21b426cb4dd6: Waiting\n",
      "6ecb67338134: Waiting\n",
      "0b98575ca820: Waiting\n",
      "1536f6ca931b: Waiting\n",
      "73eef93b7466: Waiting\n",
      "d5a54c1fb97f: Waiting\n",
      "d7b631d130cb: Waiting\n",
      "75ffe8dfb222: Waiting\n",
      "86b4bf2f8d5f: Waiting\n",
      "a613a9b4553c: Verifying Checksum\n",
      "a613a9b4553c: Download complete\n",
      "acc000f01536: Verifying Checksum\n",
      "acc000f01536: Download complete\n",
      "73eef93b7466: Verifying Checksum\n",
      "73eef93b7466: Download complete\n",
      "8e097b52bfb8: Verifying Checksum\n",
      "8e097b52bfb8: Download complete\n",
      "1536f6ca931b: Verifying Checksum\n",
      "1536f6ca931b: Download complete\n",
      "d7b631d130cb: Verifying Checksum\n",
      "d7b631d130cb: Download complete\n",
      "d5a54c1fb97f: Verifying Checksum\n",
      "d5a54c1fb97f: Download complete\n",
      "75ffe8dfb222: Download complete\n",
      "5335952fa8d3: Verifying Checksum\n",
      "5335952fa8d3: Download complete\n",
      "96fa3cc6fe10: Verifying Checksum\n",
      "96fa3cc6fe10: Download complete\n",
      "e428dd9daa94: Download complete\n",
      "1903eafd18ff: Verifying Checksum\n",
      "1903eafd18ff: Download complete\n",
      "3d15922e3194: Verifying Checksum\n",
      "3d15922e3194: Download complete\n",
      "21b426cb4dd6: Verifying Checksum\n",
      "21b426cb4dd6: Download complete\n",
      "7a5cad2d557c: Verifying Checksum\n",
      "7a5cad2d557c: Download complete\n",
      "0b98575ca820: Verifying Checksum\n",
      "0b98575ca820: Download complete\n",
      "86b4bf2f8d5f: Verifying Checksum\n",
      "86b4bf2f8d5f: Download complete\n",
      "8e097b52bfb8: Pull complete\n",
      "a613a9b4553c: Pull complete\n",
      "acc000f01536: Pull complete\n",
      "73eef93b7466: Pull complete\n",
      "6ecb67338134: Verifying Checksum\n",
      "6ecb67338134: Download complete\n",
      "d5a54c1fb97f: Pull complete\n",
      "1536f6ca931b: Pull complete\n",
      "d7b631d130cb: Pull complete\n",
      "75ffe8dfb222: Pull complete\n",
      "86b4bf2f8d5f: Pull complete\n",
      "5335952fa8d3: Pull complete\n",
      "96fa3cc6fe10: Pull complete\n",
      "e428dd9daa94: Pull complete\n",
      "1903eafd18ff: Pull complete\n",
      "3d15922e3194: Pull complete\n",
      "7a5cad2d557c: Pull complete\n",
      "21b426cb4dd6: Pull complete\n",
      "6ecb67338134: Pull complete\n",
      "0b98575ca820: Pull complete\n",
      "Digest: sha256:b8a80737a8a9923b7bc15781fcfd1ebe16c4fada68246bc4019af6d7af13fa27\n",
      "Status: Downloaded newer image for ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb:latest\n",
      "ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb:latest\n",
      "2020-11-25T11:17:20Z Check if container 9212ba86-c3d2-4914-89e8-68de3eacc10c already exist exited with 0, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt\n",
      "===============================================================================================================\n",
      "[2020-11-25T11:17:27.076407] Entering job preparation.\n",
      "[2020-11-25T11:17:27.795303] Starting job preparation.\n",
      "[2020-11-25T11:17:27.795345] Extracting the control code.\n",
      "[2020-11-25T11:17:27.814072] fetching and extracting the control code on master node.\n",
      "[2020-11-25T11:17:27.814131] Starting extract_project.\n",
      "[2020-11-25T11:17:27.814419] Starting to extract zip file.\n",
      "[2020-11-25T11:17:28.755378] Finished extracting zip file.\n",
      "[2020-11-25T11:17:28.889554] Using urllib.request Python 3.0 or later\n",
      "[2020-11-25T11:17:28.889616] Start fetching snapshots.\n",
      "[2020-11-25T11:17:28.889656] Start fetching snapshot.\n",
      "[2020-11-25T11:17:28.889668] Retrieving project from snapshot: 2e2f939c-cb97-4152-904d-7039469577eb\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 78\n",
      "[2020-11-25T11:17:29.826921] Finished fetching snapshot.\n",
      "[2020-11-25T11:17:29.826971] Start fetching snapshot.\n",
      "[2020-11-25T11:17:29.826988] Retrieving project from snapshot: 8f277172-e44d-44ee-8f2f-e8030cf6f1b6\n",
      "[2020-11-25T11:17:41.512544] Finished fetching snapshot.\n",
      "[2020-11-25T11:17:41.512583] Finished fetching snapshots.\n",
      "[2020-11-25T11:17:41.512593] Finished extract_project.\n",
      "[2020-11-25T11:17:41.524339] Finished fetching and extracting the control code.\n",
      "[2020-11-25T11:17:41.527254] downloadDataStore - Download from datastores if requested.\n",
      "[2020-11-25T11:17:41.528939] Start run_history_prep.\n",
      "[2020-11-25T11:17:41.822772] Entering context manager injector.\n",
      "Acquired lockfile /tmp/9212ba86-c3d2-4914-89e8-68de3eacc10c-datastore.lock to downloading input data references\n",
      "[2020-11-25T11:17:43.392641] downloadDataStore completed\n",
      "[2020-11-25T11:17:43.397615] Job preparation is complete.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2020/11/25 11:17:46 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2020/11/25 11:17:46 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2020-11-25T11:17:48.086472] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.18.0', '--scoring_module_name', 'batch_diabetes.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/mounts/workspaceblobstore/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/inferences', '--input_fds_0', 'diabetes_batch'])\n",
      "Initialize DatasetContextManager.\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 133\n",
      "Set Dataset diabetes_batch's target path to /tmp/tmp5jc4qns0\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.18.0.post3 azureml-dataprep==2.6.0. Session id: ee740e8f-485c-4025-b1dc-7b7381393a5d. Run id: 9212ba86-c3d2-4914-89e8-68de3eacc10c.\n",
      "Processing 'diabetes_batch'.\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'batch-data/')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"7c50a6f3-9aa4-49b1-8122-0bc3160d3615\",\n",
      "    \"name\": \"batch-data\",\n",
      "    \"version\": 1,\n",
      "    \"description\": \"batch data\",\n",
      "    \"workspace\": \"Workspace.create(name='nikhilsuthardp100', subscription_id='71bfcf50-7e10-4546-9c9a-fd4f1ee42434', resource_group='nikhil-suthardp100')\"\n",
      "  }\n",
      "}\n",
      "Mounting diabetes_batch to /tmp/tmp5jc4qns0.\n",
      "Mounted diabetes_batch to /tmp/tmp5jc4qns0 as folder.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Cannot find cached workspace due to: KeyError('71bfcf50-7e10-4546-9c9a-fd4f1ee42434nikhil-suthardp100nikhilsuthardp100',)\n",
      "Entering Run History Context Manager.\n",
      "Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/mounts/workspaceblobstore/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c\n",
      "Preparing to call script [ driver/amlbi_main.py ] with arguments: ['--client_sdk_version', '1.18.0', '--scoring_module_name', 'batch_diabetes.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/mounts/workspaceblobstore/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/inferences', '--input_fds_0', 'diabetes_batch']\n",
      "After variable expansion, calling script [ driver/amlbi_main.py ] with arguments: ['--client_sdk_version', '1.18.0', '--scoring_module_name', 'batch_diabetes.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/mounts/workspaceblobstore/azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/inferences', '--input_fds_0', 'diabetes_batch']\n",
      "\n",
      "Cannot find cached workspace due to: KeyError('71bfcf50-7e10-4546-9c9a-fd4f1ee42434nikhil-suthardp100nikhilsuthardp100',)\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 133\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt\n",
      "===============================================================================================================\n",
      "[2020-11-25T11:19:10.142665] Entering job release\n",
      "[2020-11-25T11:19:11.114590] Starting job release\n",
      "[2020-11-25T11:19:11.115564] Logging experiment finalizing status in history service.\n",
      "[2020-11-25T11:19:11.115710] job release stage : upload_datastore starting...\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 688\n",
      "[2020-11-25T11:19:11.116653] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2020-11-25T11:19:11.120373] job release stage : execute_job_release starting...\n",
      "[2020-11-25T11:19:11.120815] job release stage : copy_batchai_cached_logs starting...\n",
      "[2020-11-25T11:19:11.121788] Entering context manager injector.\n",
      "[2020-11-25T11:19:11.122001] job release stage : copy_batchai_cached_logs completed...\n",
      "[2020-11-25T11:19:11.382390] job release stage : send_run_telemetry starting...\n",
      "[2020-11-25T11:19:11.572712] job release stage : upload_datastore completed...\n",
      "[2020-11-25T11:19:12.038873] job release stage : execute_job_release completed...\n",
      "[2020-11-25T11:19:13.227632] job release stage : send_run_telemetry completed...\n",
      "[2020-11-25T11:19:13.228054] Job release is complete\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt\n",
      "===============================================================================================================\n",
      "[2020-11-25T11:19:10.450427] Entering job release\n",
      "[2020-11-25T11:19:11.491984] job release stage : copy_batchai_cached_logs starting...\n",
      "[2020-11-25T11:19:11.492036] job release stage : copy_batchai_cached_logs completed...\n",
      "\n",
      "StepRun(batch-score-diabetes) Execution Summary\n",
      "================================================\n",
      "StepRun( batch-score-diabetes ) Status: Finished\n",
      "{'runId': '9212ba86-c3d2-4914-89e8-68de3eacc10c', 'target': 'nikhilvmcluster', 'status': 'Completed', 'startTimeUtc': '2020-11-25T11:16:19.925391Z', 'endTimeUtc': '2020-11-25T11:19:36.120753Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '2e2f939c-cb97-4152-904d-7039469577eb', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'ba9712e0-6e09-4cc4-b589-67720d23bf7d', 'azureml.nodeid': 'e3a7cbed', 'azureml.pipelinerunid': '99449476-8765-452c-a28f-e734e6f92a0a', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.parallelrunstep': 'true'}, 'inputDatasets': [{'dataset': {'id': '7c50a6f3-9aa4-49b1-8122-0bc3160d3615'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'diabetes_batch', 'mechanism': 'Mount'}}], 'outputDatasets': [], 'runDefinition': {'script': 'driver/amlbi_main.py', 'useAbsolutePath': False, 'arguments': ['--client_sdk_version', '1.18.0', '--scoring_module_name', 'batch_diabetes.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '$AZUREML_DATAREFERENCE_inferences', '--input_fds_0', 'diabetes_batch'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'nikhilvmcluster', 'dataReferences': {'inferences': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/9212ba86-c3d2-4914-89e8-68de3eacc10c/inferences', 'pathOnCompute': 'diabetes/results', 'overwrite': False}}, 'data': {'diabetes_batch': {'dataLocation': {'dataset': {'id': '7c50a6f3-9aa4-49b1-8122-0bc3160d3615', 'name': None, 'version': '1'}, 'dataPath': None}, 'mechanism': 'Mount', 'environmentVariableName': 'diabetes_batch', 'pathOnCompute': None, 'overwrite': False}}, 'outputData': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 2, 'priority': None, 'environment': {'name': 'batch_environment', 'version': 'Autosave_2020-11-25T11:07:44Z_eaa6cc2e', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['scikit-learn', 'azureml-defaults~=1.18.0', 'azureml-core~=1.18.0', 'azureml-dataprep[fuse]']}], 'name': 'azureml_be341871f2a1af415628bd3b45b58373'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'frameworkImage': None, 'imageVersion': None, 'location': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=WJHmLfJwNbMb%2FoBvE9%2BooE5ZYjVhugJ15xBVEw4JBww%3D&st=2020-11-25T11%3A09%3A29Z&se=2020-11-25T19%3A19%3A29Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/55_azureml-execution-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt?sv=2019-02-02&sr=b&sig=oQdYorf6AkwkH6WXNfdPrgPNaFwpU6WY7aBB5z%2BAN7Y%3D&st=2020-11-25T11%3A09%3A29Z&se=2020-11-25T19%3A19%3A29Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/55_azureml-execution-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt?sv=2019-02-02&sr=b&sig=TwYlja5ojGHUbGL7XTsknm%2FAqe1Be3YTrfE7zbYmeiI%3D&st=2020-11-25T11%3A09%3A29Z&se=2020-11-25T19%3A19%3A29Z&sp=r', 'azureml-logs/65_job_prep-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/65_job_prep-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt?sv=2019-02-02&sr=b&sig=w2IwA2b0q3DZVS8mSaANWdbJD%2BXcoWCu2hsHw9ktSBg%3D&st=2020-11-25T11%3A09%3A29Z&se=2020-11-25T19%3A19%3A29Z&sp=r', 'azureml-logs/65_job_prep-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/65_job_prep-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt?sv=2019-02-02&sr=b&sig=02Vb%2Fqj%2FL6xuRutnWvfPvOmUjXtASRS1SpxTSQNDvYA%3D&st=2020-11-25T11%3A09%3A29Z&se=2020-11-25T19%3A19%3A29Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=J3%2Fk7AV1noktSlWavQV0y9K10nU28rs2qrm%2Fm9r%2F9Xc%3D&st=2020-11-25T11%3A09%3A30Z&se=2020-11-25T19%3A19%3A30Z&sp=r', 'azureml-logs/75_job_post-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/75_job_post-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt?sv=2019-02-02&sr=b&sig=CM9w8twdCunNk4d%2BmTruVyW7dP1KSYuNl0ao3Ng8iVI%3D&st=2020-11-25T11%3A09%3A30Z&se=2020-11-25T19%3A19%3A30Z&sp=r', 'azureml-logs/75_job_post-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/75_job_post-tvmps_e15e28665ad0e097097afebcb8c998a58e4f7578fb14939532ae6270bc3b1953_d.txt?sv=2019-02-02&sr=b&sig=4o3aAt3JZDVGO%2FZ2W1zg%2FigD2RD%2B%2Fx2LSYdfOvzspf0%3D&st=2020-11-25T11%3A09%3A30Z&se=2020-11-25T19%3A19%3A30Z&sp=r', 'azureml-logs/process_info.json': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=2gn478hInOrhYAoXjyXxXt%2BdOzVKgZ4r0bXAH1cYFrk%3D&st=2020-11-25T11%3A09%3A30Z&se=2020-11-25T19%3A19%3A30Z&sp=r', 'azureml-logs/process_status.json': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=%2FG60Jq1YD7tyG5UMNKM41YZphVurjCQ2zXH9p4spJk4%3D&st=2020-11-25T11%3A09%3A30Z&se=2020-11-25T19%3A19%3A30Z&sp=r', 'logs/azureml/119_azureml.log': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/119_azureml.log?sv=2019-02-02&sr=b&sig=KXvf5Nx7YHza%2FUbcny55xsy9o3nQI6KCFHkhWuoTHsw%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/133_azureml.log': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/133_azureml.log?sv=2019-02-02&sr=b&sig=8Gy%2FqRT0jeOWgItrNe1GJ78j3CLjX2%2BhGz0zVDpSIEs%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=PDTHr2K2TylAHSopcVqcnBdKZk%2BT7JFh%2B0CSJyxoXhA%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=U5b177YwNC%2Ftez26ioaF0cIZei%2BKnptZMIxN0jW%2BA9E%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/engine_spans_l_7d281497-60dd-4f84-bb81-1e97edae0a11.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/engine_spans_l_7d281497-60dd-4f84-bb81-1e97edae0a11.jsonl?sv=2019-02-02&sr=b&sig=Cz7IGQu%2BSHauZ29cFgyrOHiQfkQYiWckm6gcueysVzw%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/engine_spans_l_b59a460b-9f7a-46d2-93f6-7096801a5313.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/engine_spans_l_b59a460b-9f7a-46d2-93f6-7096801a5313.jsonl?sv=2019-02-02&sr=b&sig=DGPkZ210EP3N5QdE9yqxtlpS6vlYbXvI%2BFYmXcOXqdY%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/engine_spans_l_edc289af-94e5-411d-b020-8e5920de0046.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/engine_spans_l_edc289af-94e5-411d-b020-8e5920de0046.jsonl?sv=2019-02-02&sr=b&sig=h1Cn56A3UXlbwvohPgh4wagGGguemDdPFLEcLMsHqDY%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/engine_spans_l_fdc5ff19-257a-45ac-8ec3-cf1516c4ca06.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/engine_spans_l_fdc5ff19-257a-45ac-8ec3-cf1516c4ca06.jsonl?sv=2019-02-02&sr=b&sig=tK0VMAwYqpQ7H8KxkxRZRGo%2BK5lzffgughvP9nBqgtw%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_3c8643b7-4701-4820-9649-4442731ea7e7.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_3c8643b7-4701-4820-9649-4442731ea7e7.jsonl?sv=2019-02-02&sr=b&sig=jySNPetky79d46gM32m6uXYCvqQCW%2FJVEO5Yi4bnOFg%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_86af9a95-125b-4349-9d44-a7e3bd11bcea.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_86af9a95-125b-4349-9d44-a7e3bd11bcea.jsonl?sv=2019-02-02&sr=b&sig=HmcsbusJBaxaJ2LRsyPWHZiQhhs2UM5Fu4frW37Xt%2Bg%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_d2291ae0-8865-4283-bc9a-6eaee71b2e8f.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_d2291ae0-8865-4283-bc9a-6eaee71b2e8f.jsonl?sv=2019-02-02&sr=b&sig=QIEqlGmr44CxaDmmWLOVYNW1kwENSgJrrB7AG1YM2RM%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_l_7d281497-60dd-4f84-bb81-1e97edae0a11.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_l_7d281497-60dd-4f84-bb81-1e97edae0a11.jsonl?sv=2019-02-02&sr=b&sig=ztBGTO33%2BctLLabn21836mFPHOqYGDa2syrGaXx0G7g%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_l_9696107e-5a03-472d-bcb4-daafc7c6c3f2.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_l_9696107e-5a03-472d-bcb4-daafc7c6c3f2.jsonl?sv=2019-02-02&sr=b&sig=h5VWHDSsCEvPMZBajkchUBsyLl1zpypncooIfDBabH8%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_l_b59a460b-9f7a-46d2-93f6-7096801a5313.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_l_b59a460b-9f7a-46d2-93f6-7096801a5313.jsonl?sv=2019-02-02&sr=b&sig=6SUPKkChfO3E4wmSJ8Varw8uHcHAIGqZd6DkQltmrww%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_l_edc289af-94e5-411d-b020-8e5920de0046.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_l_edc289af-94e5-411d-b020-8e5920de0046.jsonl?sv=2019-02-02&sr=b&sig=shLJNrpuJY7Tc34rzb8ppmmSZ6q2uN6dwEl1vvEE9jw%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_l_f9165b3c-aa8d-4293-b386-c75b0a60b55f.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_l_f9165b3c-aa8d-4293-b386-c75b0a60b55f.jsonl?sv=2019-02-02&sr=b&sig=gn13UMMQ2rwvR2s%2BaK7Ap7cUPnZfl5NEnzmDWRX%2FMxs%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/dataprep/python_span_l_fdc5ff19-257a-45ac-8ec3-cf1516c4ca06.jsonl': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/dataprep/python_span_l_fdc5ff19-257a-45ac-8ec3-cf1516c4ca06.jsonl?sv=2019-02-02&sr=b&sig=Zp%2BKEBTM9hLykhpQRWg6vGa%2BOwY8blbpdeTKXFFOWqM%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=iR99KN45T5qhm1nzzRjPNKqNTgFxcmvDZHPN6DwKvLc%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=LzynsSnAIsgz40RF2oIsMtZbX%2FpprsAmC2yWzoEGb4o%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=tH3aHDMlLpQAS7DDXpexhRdfB7DymjBwOTGDtgRbpYQ%3D&st=2020-11-25T11%3A09%3A16Z&se=2020-11-25T19%3A19%3A16Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=aVKot%2FOefV%2BeSF6bTgVd2%2FfSfT%2FfZSs3UpjuJO69gls%3D&st=2020-11-25T11%3A09%3A17Z&se=2020-11-25T19%3A19%3A17Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.9212ba86-c3d2-4914-89e8-68de3eacc10c/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=8qOTSGSD4ktO5ybEazyfKBQyYHhu56ujHhkMDkWoyOA%3D&st=2020-11-25T11%3A09%3A17Z&se=2020-11-25T19%3A19%3A17Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '99449476-8765-452c-a28f-e734e6f92a0a', 'status': 'Completed', 'startTimeUtc': '2020-11-25T11:06:18.888818Z', 'endTimeUtc': '2020-11-25T11:19:39.506076Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.99449476-8765-452c-a28f-e734e6f92a0a/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=3SJp6dzYmHogZH3tORw0ws4giNwkH7F6J3akDY4O73s%3D&st=2020-11-25T10%3A56%3A29Z&se=2020-11-25T19%3A06%3A29Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.99449476-8765-452c-a28f-e734e6f92a0a/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=yLS8aJzxaWAn488OJJxcS5ZiD18zkAcj3PcRFS%2FIzkw%3D&st=2020-11-25T10%3A56%3A29Z&se=2020-11-25T19%3A06%3A29Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://nikhilsuthardp7211953111.blob.core.windows.net/azureml/ExperimentRun/dcid.99449476-8765-452c-a28f-e734e6f92a0a/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=NT5OKjWbpkoKnYLA62mpWX9qrhCzPBMxzHMlKqMwzjw%3D&st=2020-11-25T10%3A56%3A29Z&se=2020-11-25T19%3A06%3A29Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "print('Running pipeline...')\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the pipeline has finished running, the resulting predictions will have been saved in the outputs of the experiment associated with the first (and only) step in the pipeline. You can retrieve it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>90.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>91.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>92.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>93.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>94.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      File  Prediction\n",
       "0   22.csv           1\n",
       "1   23.csv           0\n",
       "2   24.csv           0\n",
       "3   25.csv           0\n",
       "4   26.csv           1\n",
       "5   59.csv           0\n",
       "6    6.csv           0\n",
       "7   60.csv           1\n",
       "8   61.csv           0\n",
       "9   62.csv           0\n",
       "10  18.csv           0\n",
       "11  19.csv           0\n",
       "12   2.csv           0\n",
       "13  20.csv           0\n",
       "14  21.csv           1\n",
       "15  90.csv           0\n",
       "16  91.csv           0\n",
       "17  92.csv           0\n",
       "18  93.csv           1\n",
       "19  94.csv           0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
    "\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='diabetes-results')\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk('diabetes-results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# cleanup output format\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "\n",
    "# Display the first 20 results\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the Pipeline and use its REST Interface\n",
    "\n",
    "Now that you have a working pipeline for batch inferencing, you can publish it and use a REST endpoint to run it from an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>Diabetes_Parallel_Batch_Pipeline</td><td><a href=\"https://ml.azure.com/pipelines/22a31c28-54cd-4a9c-ad20-bb9b3d7c1610?wsid=/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourcegroups/nikhil-suthardp100/workspaces/nikhilsuthardp100\" target=\"_blank\" rel=\"noopener\">22a31c28-54cd-4a9c-ad20-bb9b3d7c1610</a></td><td>Active</td><td><a href=\"https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourceGroups/nikhil-suthardp100/providers/Microsoft.MachineLearningServices/workspaces/nikhilsuthardp100/PipelineRuns/PipelineSubmit/22a31c28-54cd-4a9c-ad20-bb9b3d7c1610\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: Diabetes_Parallel_Batch_Pipeline,\n",
       "Id: 22a31c28-54cd-4a9c-ad20-bb9b3d7c1610,\n",
       "Status: Active,\n",
       "Endpoint: https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourceGroups/nikhil-suthardp100/providers/Microsoft.MachineLearningServices/workspaces/nikhilsuthardp100/PipelineRuns/PipelineSubmit/22a31c28-54cd-4a9c-ad20-bb9b3d7c1610)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name='Diabetes_Parallel_Batch_Pipeline', description='Batch scoring of diabetes data', version='1.0')\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the published pipeline has an endpoint, which you can see in the Azure portal. You can also find it as a property of the published pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourceGroups/nikhil-suthardp100/providers/Microsoft.MachineLearningServices/workspaces/nikhilsuthardp100/PipelineRuns/PipelineSubmit/22a31c28-54cd-4a9c-ad20-bb9b3d7c1610\n"
     ]
    }
   ],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the endpoint, client applications need to make a REST call over HTTP. This request must be authenticated, so an authorization header is required. To test this out, we'll use the authorization header from your current connection to your Azure workspace, which you can get using the following code:\n",
    "\n",
    "> **Note**: A real application would require a service principal with which to be authenticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication header ready.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "print('Authentication header ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to call the REST interface. The pipeline runs asynchronously, so we'll get an identifier back, which we can use to track the pipeline experiment as it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48a0bd37-da94-4422-bcab-d861c926d9ac'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": \"Batch_Pipeline_via_REST\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the run ID, we can view the experiment as it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline...\n",
      "PipelineRunId: 48a0bd37-da94-4422-bcab-d861c926d9ac\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Batch_Pipeline_via_REST/runs/48a0bd37-da94-4422-bcab-d861c926d9ac?wsid=/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourcegroups/nikhil-suthardp100/workspaces/nikhilsuthardp100\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: ee745dfb-5d31-4584-830e-145f4da50a01\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Batch_Pipeline_via_REST/runs/ee745dfb-5d31-4584-830e-145f4da50a01?wsid=/subscriptions/71bfcf50-7e10-4546-9c9a-fd4f1ee42434/resourcegroups/nikhil-suthardp100/workspaces/nikhilsuthardp100\n",
      "StepRun( batch-score-diabetes ) Status: NotStarted\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt\n",
      "========================================================================================================================\n",
      "2020-11-25T11:23:07Z Starting output-watcher...\n",
      "2020-11-25T11:23:07Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_204af3dadda12064de42b82ef2062eeb\n",
      "Digest: sha256:b8a80737a8a9923b7bc15781fcfd1ebe16c4fada68246bc4019af6d7af13fa27\n",
      "Status: Image is up to date for ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb:latest\n",
      "ed4462f89ccc45679b9210c6758878a5.azurecr.io/azureml/azureml_204af3dadda12064de42b82ef2062eeb:latest\n",
      "StepRun( batch-score-diabetes ) Status: Running\n",
      "2020-11-25T11:23:08Z Check if container ee745dfb-5d31-4584-830e-145f4da50a01 already exist exited with 0, \n",
      "\n",
      "363a7f631951e9bbbd583834eb1b77c387ae6c1d2bb8423bd74e47d00b3ed0ce\n",
      "2020/11/25 11:23:10 Starting App Insight Logger for task:  containerSetup\n",
      "2020/11/25 11:23:10 Version: 3.0.01417.0012 Branch: 56 Commit: d244ddd\n",
      "2020/11/25 11:23:10 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/11/25 11:23:10 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/11/25 11:23:10 sshd runtime has already been installed in the container\n",
      "ssh-keygen: /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "ssh-keygen: /azureml-envs/azureml_be341871f2a1af415628bd3b45b58373/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "2020/11/25 11:23:10 All App Insights Logs was send successfully\n",
      "2020-11-25T11:23:11Z Starting docker container succeeded.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2020/11/25 11:23:31 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2020/11/25 11:23:31 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2020-11-25T11:23:32.509293] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.18.0', '--scoring_module_name', 'batch_diabetes.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/ee745dfb-5d31-4584-830e-145f4da50a01/mounts/workspaceblobstore/azureml/ee745dfb-5d31-4584-830e-145f4da50a01/inferences', '--input_fds_0', 'diabetes_batch'])\n",
      "Initialize DatasetContextManager.\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 134\n",
      "Set Dataset diabetes_batch's target path to /tmp/tmpm6a2asf5\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.18.0.post3 azureml-dataprep==2.6.0. Session id: 1b1a4ad9-6d5a-4180-a70b-55cabcbf87f1. Run id: ee745dfb-5d31-4584-830e-145f4da50a01.\n",
      "Processing 'diabetes_batch'.\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'batch-data/')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"7c50a6f3-9aa4-49b1-8122-0bc3160d3615\",\n",
      "    \"name\": \"batch-data\",\n",
      "    \"version\": 1,\n",
      "    \"description\": \"batch data\",\n",
      "    \"workspace\": \"Workspace.create(name='nikhilsuthardp100', subscription_id='71bfcf50-7e10-4546-9c9a-fd4f1ee42434', resource_group='nikhil-suthardp100')\"\n",
      "  }\n",
      "}\n",
      "Mounting diabetes_batch to /tmp/tmpm6a2asf5.\n",
      "Mounted diabetes_batch to /tmp/tmpm6a2asf5 as folder.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Entering Run History Context Manager.\n",
      "Cannot find cached workspace due to: KeyError('71bfcf50-7e10-4546-9c9a-fd4f1ee42434nikhil-suthardp100nikhilsuthardp100',)\n",
      "Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/ee745dfb-5d31-4584-830e-145f4da50a01/mounts/workspaceblobstore/azureml/ee745dfb-5d31-4584-830e-145f4da50a01\n",
      "Preparing to call script [ driver/amlbi_main.py ] with arguments: ['--client_sdk_version', '1.18.0', '--scoring_module_name', 'batch_diabetes.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/ee745dfb-5d31-4584-830e-145f4da50a01/mounts/workspaceblobstore/azureml/ee745dfb-5d31-4584-830e-145f4da50a01/inferences', '--input_fds_0', 'diabetes_batch']\n",
      "After variable expansion, calling script [ driver/amlbi_main.py ] with arguments: ['--client_sdk_version', '1.18.0', '--scoring_module_name', 'batch_diabetes.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/nikhilsuthardp100/azureml/ee745dfb-5d31-4584-830e-145f4da50a01/mounts/workspaceblobstore/azureml/ee745dfb-5d31-4584-830e-145f4da50a01/inferences', '--input_fds_0', 'diabetes_batch']\n",
      "\n",
      "Cannot find cached workspace due to: KeyError('71bfcf50-7e10-4546-9c9a-fd4f1ee42434nikhil-suthardp100nikhilsuthardp100',)\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_9ca2d4dc7491f214fa6ce9e3f79cdb41d04d2fe8b3c333575f6a1616047f1e2e_d.txt\n",
      "===============================================================================================================\n",
      "[2020-11-25T11:25:02.810926] Entering job release\n",
      "[2020-11-25T11:25:03.759533] job release stage : copy_batchai_cached_logs starting...\n",
      "[2020-11-25T11:25:03.759587] job release stage : copy_batchai_cached_logs completed...\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"Batch_Pipeline_via_REST\"], run_id)\n",
    "print('Running pipeline...')\n",
    "published_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the results are in the output of the first pipeline step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"diabetes-results\", ignore_errors=True)\n",
    "\n",
    "prediction_run = next(published_pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"inferences\")\n",
    "prediction_output.download(local_path=\"diabetes-results\")\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"diabetes-results\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# cleanup output format\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "\n",
    "# Display the first 20 results\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a pipeline that can be used to batch process daily patient data.\n",
    "\n",
    "**More Information**: For more details about using pipelines for batch inferencing, see the [How to Run Batch Predictions](https://docs.microsoft.com/azure/machine-learning/how-to-run-batch-predictions) in the Azure Machine Learning documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
